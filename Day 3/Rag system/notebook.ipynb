{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79906239",
   "metadata": {},
   "source": [
    "## Testing ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e914eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can be used in a variety of ways, from helping you plan a vacation to creating art. I'm here to assist you in finding the help or information you need. My strengths include answering questions, generating text and images, as well as being able to play games with you.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    " # Test the Ollama API\n",
    "response = requests.post(\n",
    "    \"http://localhost:11434/api/generate\",\n",
    "    json={\n",
    "        \"model\": \"llama3.2:1b\",\n",
    "        \"prompt\": \"Hello, what are your capabilities?\",\n",
    "        \"stream\": False\n",
    "    }\n",
    ")\n",
    "print(response.json()[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5d64f7",
   "metadata": {},
   "source": [
    "## Document preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69cf9cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2 documents into 6 chunks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from rag.document_processor import DocumentProcessor\n",
    " # Create a test document\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "with open(\"data/test_document.txt\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "    Retrieval-Augmented Generation (RAG) is a technique that enhances large language models\n",
    "    by allowing them to access external knowledge. This approach combines the strengths of \n",
    "    retrieval-based and generation-based methods in natural language processing.\n",
    "    \n",
    "    The key components of a RAG system include:\n",
    "    1. A document store containing knowledge\n",
    "    2. A retrieval system to find relevant information\n",
    "    3. A language model to generate responses\n",
    "    \n",
    "    RAG addresses the limitations of traditional language models, such as outdated knowledge\n",
    "    and hallucinations, by grounding responses in factual information from external sources.\n",
    "    \"\"\")\n",
    " # Initialize the document processor\n",
    " # Note: Using smaller chunks for Llama 3.2 1B to accommodate its smaller context window\n",
    "processor = DocumentProcessor(chunk_size=500, chunk_overlap=50)\n",
    " # Process the test document\n",
    "chunks = processor.process_documents(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aacda21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 6\n",
      "\n",
      "Chunk 1:\n",
      "Text: # Retrieval-Augmented Generation (RAG)\n",
      "        \n",
      "        Retrieval-Augmented Generation (RAG) is an AI framework that enhances large language models\n",
      "        by incorporating external knowledge retrieval into the generation process. It was introduced\n",
      "        by researchers at Facebook AI in 2020.\n",
      "        \n",
      "        ## Core Components\n",
      "        \n",
      "        1. **Document Store**: A collection of documents containing domain-specific knowledge....\n",
      "Metadata: {'source': 'data\\\\rag_explanation.txt'}\n",
      "\n",
      "Chunk 2:\n",
      "Text: 2. **Retriever**: A system that finds relevant documents or passages based on a query.\n",
      "        3. **Generator**: A language model that produces responses using the retrieved information.\n",
      "        4. **Embedding Model**: Converts text into vector representations for similarity matching.\n",
      "        5. **Vector Database**: Efficiently stores and indexes embeddings for quick retrieval.\n",
      "        \n",
      "        ## Advantages of RAG...\n",
      "Metadata: {'source': 'data\\\\rag_explanation.txt'}\n",
      "\n",
      "Chunk 3:\n",
      "Text: ## Advantages of RAG\n",
      "        \n",
      "        - Reduces hallucinations by grounding responses in factual information\n",
      "        - Enables access to up-to-date information beyond the model's training data\n",
      "        - Allows incorporation of domain-specific knowledge\n",
      "        - Provides transparency through explicit source attribution\n",
      "        - More cost-effective than continuous model retraining\n",
      "        \n",
      "        ## Implementation Approaches...\n",
      "Metadata: {'source': 'data\\\\rag_explanation.txt'}\n",
      "\n",
      "Chunk 4:\n",
      "Text: ## Implementation Approaches\n",
      "        \n",
      "        There are several ways to implement RAG systems:\n",
      "        \n",
      "        - **Basic RAG**: Simple retrieval followed by generation\n",
      "        - **Advanced RAG**: Includes query reformulation, multi-step retrieval, and reranking\n",
      "        - **Hybrid Approaches**: Combines fine-tuning with retrieval for specialized domains...\n",
      "Metadata: {'source': 'data\\\\rag_explanation.txt'}\n",
      "\n",
      "Chunk 5:\n",
      "Text: Retrieval-Augmented Generation (RAG) is a technique that enhances large language models\n",
      "    by allowing them to access external knowledge. This approach combines the strengths of \n",
      "    retrieval-based and generation-based methods in natural language processing.\n",
      "    \n",
      "    The key components of a RAG system include:\n",
      "    1. A document store containing knowledge\n",
      "    2. A retrieval system to find relevant information\n",
      "    3. A language model to generate responses...\n",
      "Metadata: {'source': 'data\\\\test_document.txt'}\n",
      "\n",
      "Chunk 6:\n",
      "Text: RAG addresses the limitations of traditional language models, such as outdated knowledge\n",
      "    and hallucinations, by grounding responses in factual information from external sources....\n",
      "Metadata: {'source': 'data\\\\test_document.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"Text: {chunk.page_content}...\")\n",
    "    print(f\"Metadata: {chunk.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9850c6d",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5fee902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2 documents into 6 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\projects\\notebooks\\ollama env\\rag\\embeddings.py:20: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
      "c:\\projects\\notebooks\\ollama env\\ollama-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\projects\\notebooks\\ollama env\\rag\\embeddings.py:42: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  self.vectorstore.persist()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created vector store with 6 documents\n",
      "Loaded vector store from vectorstore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\projects\\notebooks\\ollama env\\rag\\embeddings.py:58: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  self.vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "from rag.document_processor import DocumentProcessor\n",
    "from rag.embeddings import EmbeddingManager\n",
    "import os\n",
    " # Process documents\n",
    "processor = DocumentProcessor(chunk_size=500, chunk_overlap=50)\n",
    "chunks = processor.process_documents(\"data\")\n",
    " # Create a persist directory\n",
    "os.makedirs(\"vectorstore\", exist_ok=True)\n",
    " # Initialize embedding manager with local embeddings\n",
    "embedding_manager = EmbeddingManager(\n",
    "    model_name=\"all-MiniLM-L6-v2\",  # Lightweight but effective embedding model\n",
    "    persist_directory=\"vectorstore\"\n",
    ")\n",
    " # Create vector store from documents\n",
    "embedding_manager.create_vectorstore(chunks)\n",
    " # Test loading the vector store\n",
    "embedding_manager = EmbeddingManager(\n",
    "    model_name=\"all-MiniLM-L6-v2\",\n",
    "    persist_directory=\"vectorstore\"\n",
    ")\n",
    "success = embedding_manager.load_vectorstore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d23c4eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store loaded successfully: True\n",
      "Vector store contains approximately 20 documents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vector store loaded successfully: {success}\")\n",
    " # Get the vector store\n",
    "vectorstore = embedding_manager.get_vectorstore()\n",
    "print(f\"Vector store contains approximately {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3b2971",
   "metadata": {},
   "source": [
    "## Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31d91e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2 documents into 6 chunks\n",
      "Created vector store with 6 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\projects\\notebooks\\ollama env\\rag\\retriever.py:30: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  documents = self.retriever.get_relevant_documents(query)\n"
     ]
    }
   ],
   "source": [
    "from rag.document_processor import DocumentProcessor\n",
    "from rag.embeddings import EmbeddingManager\n",
    "from rag.retriever import Retriever\n",
    " # Process documents (using previous example data)\n",
    "processor = DocumentProcessor(chunk_size=500, chunk_overlap=50)\n",
    "chunks = processor.process_documents(\"data\")\n",
    " # Create embedding manager and vector store\n",
    "embedding_manager = EmbeddingManager(model_name=\"all-MiniLM-L6-v2\")\n",
    "embedding_manager.create_vectorstore(chunks)\n",
    "vectorstore = embedding_manager.get_vectorstore()\n",
    " # Initialize retriever\n",
    "retriever = Retriever(vectorstore, top_k=2)\n",
    " # Test simple retrieval\n",
    "query = \"What is RAG and what are its components?\"\n",
    "documents = retriever.retrieve(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21d5cb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is RAG and what are its components?\n",
      "Retrieved 2 documents:\n",
      "\n",
      "Document 1:\n",
      "\n",
      "Document 2:\n",
      "Content: Retrieval-Augmented Generation (RAG) is a technique that enhances large language models\n",
      "    by allowing them to access external knowledge. This approa...\n",
      "Source: data\\test_document.txt\n",
      "\n",
      "Documents with similarity scores:\n",
      "Document 1 - Score: 0.8775\n",
      "Document 2 - Score: 1.0964\n",
      "\n",
      "MMR retrieval results:\n",
      "Document 1: ## Implementation Approaches\n",
      "        \n",
      "        There are several ways to implement RAG systems:\n",
      "        \n",
      "        - **Basic RAG**: Simple retrieval followed by generation\n",
      "        - **Advanced RAG**: Includes query reformulation, multi-step retrieval, and reranking\n",
      "        - **Hybrid Approaches**: Combines fine-tuning with retrieval for specialized domains...\n",
      "Document 2: RAG addresses the limitations of traditional language models, such as outdated knowledge\n",
      "    and hallucinations, by grounding responses in factual information from external sources....\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query: {query}\")\n",
    "print(f\"Retrieved {len(documents)} documents:\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "print(f\"Content: {doc.page_content[:150]}...\")\n",
    "print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "# Test retrieval with scores\n",
    "documents_with_scores = retriever.retrieve_with_scores(query)\n",
    "print(\"\\nDocuments with similarity scores:\")\n",
    "for i, (doc, score) in enumerate(documents_with_scores):\n",
    "    print(f\"Document {i+1} - Score: {score:.4f}\")\n",
    "# Test MMR retrieval for diversity\n",
    "mmr_documents = retriever.retrieve_with_mmr(query, diversity=0.7)\n",
    "print(\"\\nMMR retrieval results:\")\n",
    "for i, doc in enumerate(mmr_documents):\n",
    "    print(f\"Document {i+1}: {doc.page_content}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9caab8c",
   "metadata": {},
   "source": [
    "# RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1c7ef28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vector store from vectorstore\n",
      "Connected to Ollama with model: llama3.2:1b\n",
      "Ollama RAG system initialized successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\projects\\notebooks\\ollama env\\rag\\generator.py:22: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  self.llm = Ollama(model=model_name, temperature=temperature)\n"
     ]
    }
   ],
   "source": [
    "from main import OllamaRAGSystem\n",
    "import os\n",
    " # Make sure we have test data\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "test_file_path = \"data/rag_explanation.txt\"\n",
    "if not os.path.exists(test_file_path):\n",
    "    with open(test_file_path, \"w\") as f:\n",
    "        f.write(\"\"\"\n",
    "        # Retrieval-Augmented Generation (RAG)\n",
    "        \n",
    "        Retrieval-Augmented Generation (RAG) is an AI framework that enhances large language models\n",
    "        by incorporating external knowledge retrieval into the generation process. It was introduced\n",
    "        by researchers at Facebook AI in the year 2020.\n",
    "        \n",
    "        ## Core Components\n",
    "        \n",
    "        1. **Document Store**: A collection of documents containing domain-specific knowledge.\n",
    "        2. **Retriever**: A system that finds relevant documents or passages based on a query.\n",
    "        3. **Generator**: A language model that produces responses using the retrieved information.\n",
    "        4. **Embedding Model**: Converts text into vector representations for similarity matching.\n",
    "        5. **Vector Database**: Efficiently stores and indexes embeddings for quick retrieval.\n",
    "        \n",
    "        ## Advantages of RAG\n",
    "        \n",
    "        - Reduces hallucinations by grounding responses in factual information\n",
    "        - Enables access to up-to-date information beyond the model's training data\n",
    "        - Allows incorporation of domain-specific knowledge\n",
    "        - Provides transparency through explicit source attribution\n",
    "        - More cost-effective than continuous model retraining\n",
    "        \n",
    "        ## Implementation Approaches\n",
    "        \n",
    "        There are several ways to implement RAG systems:\n",
    "        \n",
    "        - **Basic RAG**: Simple retrieval followed by generation\n",
    "        - **Advanced RAG**: Includes query reformulation, multi-step retrieval, and reranking\n",
    "        - **Hybrid Approaches**: Combines fine-tuning with retrieval for specialized domains\n",
    "        \"\"\")\n",
    " # Initialize the RAG system with Llama 3.2 1B\n",
    "rag = OllamaRAGSystem(\n",
    "    data_dir=\"data\",\n",
    "    ollama_model=\"llama3.2:1b\",\n",
    "    top_k=2  # Using a smaller top_k value due to Llama 3.2's limited context window\n",
    ")\n",
    " # Test queries\n",
    "queries = [\n",
    "    \"What is RAG and when was it introduced?\",\n",
    "    \"What are the main components of a RAG system?\",\n",
    "    \"What are the advantages of using RAG over traditional LLMs?\",\n",
    "    \"How can RAG be implemented in practice?\"\n",
    " ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e99fe1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Query: What is RAG and when was it introduced?\n",
      "================================================================================\n",
      "\n",
      "Response:\n",
      "Based on the provided documents, RAG (Retrieval-Augmented Generation) systems were first introduced in [Document 1] as a basic approach to implementing Retrieval-Augmented Generation (RAG) systems.\n",
      "\n",
      "MMR Response:\n",
      "Based on the provided documents, RAG (Retrieval-Augmented Generation) addresses the limitations of traditional language models [Document 1] by grounding responses in factual information from external sources [Document 2]. \n",
      "\n",
      "RAG was introduced as an advanced retrieval approach [Document 2], which includes query reformulation and reranking to improve its performance.\n",
      "\n",
      "================================================================================\n",
      "Query: What are the main components of a RAG system?\n",
      "================================================================================\n",
      "\n",
      "Response:\n",
      "Based on the provided documents, the main components of a RAG (Retrieval-Augmented Generation) system include:\n",
      "\n",
      "1. **Retrieval**: This involves finding relevant documents in a database or data source.\n",
      "2. **Generation**: This involves generating new content, such as text or images, based on the retrieved documents.\n",
      "\n",
      "These two components are often combined to create a hybrid approach, where retrieval is used to find relevant documents and generation is used to create new content [Document 1: Implementation Approaches; Document 2: Implementation Approaches].\n",
      "\n",
      "MMR Response:\n",
      "Based on the provided documents, the main components of a Retrieval-Augmented Generation (RAG) system include:\n",
      "\n",
      "1. A document store containing knowledge [Document 1] [Document 2]\n",
      "2. A retrieval system to find relevant information\n",
      "3. A language model to generate responses\n",
      "\n",
      "I don't know based on the provided information\n",
      "\n",
      "================================================================================\n",
      "Query: What are the advantages of using RAG over traditional LLMs?\n",
      "================================================================================\n",
      "\n",
      "Response:\n",
      "Based on the provided documents, it appears that RAG (Retrieval-Augmented Generation) systems offer several advantages over traditional Large Language Models (LLMs). [Document 1] mentions that Basic RAG provides a simple retrieval followed by generation, which can be beneficial for tasks like text summarization and question answering. Additionally, Advanced RAG includes query reformulation, multi-step retrieval, and reranking, which can improve the accuracy of generated responses.\n",
      "\n",
      "[Document 2] also highlights the benefits of RAG systems, stating that they combine fine-tuning with retrieval for specialized domains. This approach allows RAG systems to adapt to specific domains and tasks more effectively than traditional LLMs.\n",
      "\n",
      "Overall, it seems that RAG systems offer a more flexible and effective way to generate text compared to traditional LLMs, particularly in specialized domains where domain-specific knowledge is crucial.\n",
      "\n",
      "MMR Response:\n",
      "Based on the provided documents, the advantages of using RAG (Retrieval-Augmented Generation) systems over traditional Large Language Models (LLMs) include:\n",
      "\n",
      "- Reduced hallucinations by grounding responses in factual information [Document 1]\n",
      "- Enables access to up-to-date information beyond the model's training data\n",
      "- Allows incorporation of domain-specific knowledge\n",
      "- Provides transparency through explicit source attribution\n",
      "\n",
      "These advantages highlight the benefits of RAG systems, which can improve the accuracy and reliability of generated text.\n",
      "\n",
      "================================================================================\n",
      "Query: How can RAG be implemented in practice?\n",
      "================================================================================\n",
      "\n",
      "Response:\n",
      "Based on the provided documents, RAG (Retrieval-Augmented Generation) systems can be implemented in practice through various approaches. Here are some possible ways:\n",
      "\n",
      "1. **Hybrid Approaches**: Combine fine-tuning with retrieval for specialized domains [Document 2]. This involves training a model to generate text based on retrieved documents and then fine-tuning it for specific tasks.\n",
      "2. **Advanced RAG**: Includes query reformulation, multi-step retrieval, and reranking [Document 1]. This approach can be used to improve the performance of basic RAG systems by incorporating additional steps such as query reformulation and reranking.\n",
      "3. **Fine-Tuning with Retrieval**: Train a model on a large dataset of retrieved documents and then fine-tune it for specific tasks [Document 2]. This involves using pre-trained language models or fine-tuning a pre-trained model on a specific task.\n",
      "\n",
      "I don't know based on the provided information\n",
      "\n",
      "MMR Response:\n",
      "Based on the provided documents, implementing RAG systems in practice involves several steps:\n",
      "\n",
      "1. **Knowledge Retrieval**: Use a reliable knowledge retrieval system to gather factual information from external sources [Document 1][1].\n",
      "2. **Query Reformulation**: Reformat queries to make them more effective and efficient for retrieval [Document 1][2].\n",
      "3. **Reranking**: Apply reranking techniques to improve the ranking of retrieved documents based on their relevance and accuracy [Document 1][3].\n",
      "4. **Fine-Tuning**: Fine-tune the model with additional training data or fine-tuning objectives to improve its performance in specialized domains [Document 2][4].\n",
      "\n",
      "By following these steps, RAG systems can be effectively implemented in practice.\n",
      "\n",
      "I don't know based on the provided information\n"
     ]
    }
   ],
   "source": [
    " # Process each query and display results\n",
    "for query in queries:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get response with sources\n",
    "    result = rag.query(query, with_sources=True)\n",
    "    \n",
    "    print(f\"\\nResponse:\\n{result['response']}\")\n",
    "    \n",
    "    # Test MMR retrieval for diverse results\n",
    "    mmr_result = rag.query(query, with_sources=True, use_mmr=True)\n",
    "    \n",
    "    print(f\"\\nMMR Response:\\n{mmr_result['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560bb787",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c9903c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vector store from vectorstore\n",
      "Connected to Ollama with model: llama3.2:1b\n",
      "Ollama RAG system initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "from main import OllamaRAGSystem\n",
    "from rag.evaluator import SimpleRAGEvaluator\n",
    "import pandas as pd\n",
    " # Initialize the RAG system\n",
    "rag = OllamaRAGSystem(\n",
    "    data_dir=\"data\",\n",
    "    ollama_model=\"llama3.2:1b\",\n",
    "    top_k=2\n",
    ")\n",
    " # Create test cases\n",
    "test_queries = [\n",
    "    {\n",
    "        \"query\": \"What is RAG and when was it introduced?\",\n",
    "        \"relevant_docs\": [\"data/rag_explanation.txt\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are the main components of a RAG system?\",\n",
    "        \"relevant_docs\": [\"data/rag_explanation.txt\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are the advantages of using RAG?\",\n",
    "        \"relevant_docs\": [\"data/rag_explanation.txt\"]\n",
    "    }\n",
    "]\n",
    " # Initialize evaluator\n",
    "evaluator = SimpleRAGEvaluator()\n",
    " # Run evaluation\n",
    "results = evaluator.run_evaluation(rag, test_queries)\n",
    " # Display results\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#print(results[[\"query\", \"response\", \"precision\", \"recall\", \"f1_score\", \"context_utilization\", \"has_citations\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe3091b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>response</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>context_utilization</th>\n",
       "      <th>has_citations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is RAG and when was it introduced?</td>\n",
       "      <td>Based on the provided documents, RAG (Retrieval-Augmented Generation) systems were first introduced in [Document 1] Implementation Approaches. Specifically, they are mentioned as a basic retrieval approach followed by generation [Document 1].</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the main components of a RAG system?</td>\n",
       "      <td>Based on the provided documents, the main components of a RAG (Retrieval-Augmented Generation) system include:\\n\\n1. **Retrieval**: This involves finding relevant documents in a database.\\n2. **Generation**: This involves generating new content based on the retrieved documents.\\n\\nThese two components are often combined to create a hybrid approach, as mentioned in [Document 2] \"Combines fine-tuning with retrieval for specialized domains\".</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the advantages of using RAG?</td>\n",
       "      <td>Based on the provided documents, it appears that RAG (Retrieval-Augmented Generation) systems offer several advantages. According to [Document 1], Basic RAG provides a simple retrieval process followed by generation, which can be beneficial for certain applications.\\n\\nHowever, as mentioned in [Document 2], Advanced RAG includes query reformulation, multi-step retrieval, and reranking, which can lead to improved results and better performance. Additionally, Hybrid Approaches combine fine-tuning with retrieval for specialized domains, suggesting that a combination of these approaches may offer the most benefits.\\n\\nIt's worth noting that the advantages of using RAG systems are not explicitly stated in either document, so I couldn't provide a definitive answer based on the provided information alone.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.356322</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           query  \\\n",
       "0        What is RAG and when was it introduced?   \n",
       "1  What are the main components of a RAG system?   \n",
       "2          What are the advantages of using RAG?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    response  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Based on the provided documents, RAG (Retrieval-Augmented Generation) systems were first introduced in [Document 1] Implementation Approaches. Specifically, they are mentioned as a basic retrieval approach followed by generation [Document 1].   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                 Based on the provided documents, the main components of a RAG (Retrieval-Augmented Generation) system include:\\n\\n1. **Retrieval**: This involves finding relevant documents in a database.\\n2. **Generation**: This involves generating new content based on the retrieved documents.\\n\\nThese two components are often combined to create a hybrid approach, as mentioned in [Document 2] \"Combines fine-tuning with retrieval for specialized domains\".   \n",
       "2  Based on the provided documents, it appears that RAG (Retrieval-Augmented Generation) systems offer several advantages. According to [Document 1], Basic RAG provides a simple retrieval process followed by generation, which can be beneficial for certain applications.\\n\\nHowever, as mentioned in [Document 2], Advanced RAG includes query reformulation, multi-step retrieval, and reranking, which can lead to improved results and better performance. Additionally, Hybrid Approaches combine fine-tuning with retrieval for specialized domains, suggesting that a combination of these approaches may offer the most benefits.\\n\\nIt's worth noting that the advantages of using RAG systems are not explicitly stated in either document, so I couldn't provide a definitive answer based on the provided information alone.   \n",
       "\n",
       "   precision  recall  f1_score  context_utilization  has_citations  \n",
       "0        0.0     0.0         0             0.500000           True  \n",
       "1        0.0     0.0         0             0.320000           True  \n",
       "2        0.0     0.0         0             0.356322           True  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[[\"query\", \"response\", \"precision\", \"recall\", \"f1_score\", \"context_utilization\", \"has_citations\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
